{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "0bc76fb2-549b-4258-be7e-db521ff179e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "0c51da33-2b4b-4111-dfb4-67e6c9cbb607"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "660d1e72-5fae-4ef2-a76c-cc8854975110"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "95731df9-a050-4edd-fe7d-171d3a00453a"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "f9dc1641-9738-4054-eaf8-bb3f2957cff9"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0, True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1CJNPhDGPA",
        "outputId": "bf28dccf-f532-441a-b32b-d9a8ccddece8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "04bdb38f-0519-4be5-b3a3-2bd56c54cb08"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1, True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "83d6e16c-ff96-4f4f-f936-3bfe111c602b"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "84f205cf-92d6-4243-c32b-14ab058581fc"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "c83e0167-d6a6-4d63-c948-eaee75a7aefb"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "ce0ae559-f28b-427d-a6d7-bdeb0bb0ec98"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 57/1000\n",
            "Average number of steps: 11.912280701754385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "6e99d38b-bacb-44a6-999d-7d778ccde740"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 90/1000\n",
            "Average number of steps: 15.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "6281fe2a-2b5a-4621-af23-6b906a9d9b8f"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 773/1000\n",
            "Average number of steps: 45.377749029754206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Evaluation"
      ],
      "metadata": {
        "id": "th1yQQoy53MG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Policy evaluation: Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "0f43a2cd-c0cf-419f-a928-b6ddd89753e7"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "858945af-bfe7-4211-aa99-678e1b957e40"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "2165142d-399f-4487-91ef-d940308a09d3"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "be73fff0-45b5-4904-d797-d46c03198e7d"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "3e9065f4-ea60-4983-afa5-fba4d45fd856"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "876f941f-a718-40dc-d571-fd6e0fe76de5"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "8aebbab7-e709-49e9-d2b3-6545d5b47707"
      },
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration"
      ],
      "metadata": {
        "id": "FZIUimzD50Mx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Value iteration: Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "bface982-2ebc-4ab8-e007-13eb2d7fedd3"
      },
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration: Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "c60c1b84-07ae-4b32-8da0-e4616cf0c819"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Extraction"
      ],
      "metadata": {
        "id": "0g54OIBQ5x8t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHYtfm4qikV",
        "outputId": "6d0d4794-c805-488c-c503-e0deca699e2a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play(env, optimal_policy, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww12Uh5qCUb",
        "outputId": "c644c03a-a026-44c0-9217-c1ce628d6957"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "6d7ea2d6-fb4d-4d78-af33-10b7c7988ae6"
      },
      "source": [
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 789/1000\n",
            "Average number of steps: 41.447401774397974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Iteration"
      ],
      "metadata": {
        "id": "xolYfArF5vUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_interation(env, max_iters=500, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    currentPolicy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "    bestPolicy = []\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        v_value = policy_evaluation(env, currentPolicy)\n",
        "        nextPolicy = policy_extraction(env, v_value)\n",
        "        if np.array_equal(nextPolicy, currentPolicy):\n",
        "          bestPolicy = currentPolicy\n",
        "          print(f'Policy iteration: Converged at {i}-th iteration.')\n",
        "          break\n",
        "        currentPolicy = nextPolicy\n",
        "    return bestPolicy"
      ],
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrozenLake-v1"
      ],
      "metadata": {
        "id": "c6ITNjpy6Giy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "valueTimes = []\n",
        "policyTimes = []"
      ],
      "metadata": {
        "id": "avv72aqf-PHm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FroLakv1 = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ],
      "metadata": {
        "id": "vzMHPru5-oNY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timeCompute(env, type='value'):\n",
        "  start, end = 0, 0\n",
        "  if type == 'value':\n",
        "    start = time.time()\n",
        "    value = value_iteration(env)\n",
        "    end = time.time()\n",
        "    policy = policy_extraction(env, value, gamma=0.9)\n",
        "  else:\n",
        "    start = time.time()\n",
        "    policy = policy_interation(env)\n",
        "    end = time.time()\n",
        "  return policy, end - start"
      ],
      "metadata": {
        "id": "aofDFwls6-UO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FroLakv1_policy, timePolicy = timeCompute(FroLakv1, 'policy')\n",
        "FroLakv1_value, timeValue = timeCompute(FroLakv1, 'value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwRNfommiUh",
        "outputId": "20c13b5a-76c8-41b6-a616-0d0f156093fd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 0-th iteration.\n",
            "Policy evaluation: Converged at 23-th iteration.\n",
            "Policy evaluation: Converged at 59-th iteration.\n",
            "Policy evaluation: Converged at 62-th iteration.\n",
            "Policy evaluation: Converged at 79-th iteration.\n",
            "Policy evaluation: Converged at 80-th iteration.\n",
            "Policy iteration: Converged at 5-th iteration.\n",
            "Value iteration: Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Value Iteration: {timeValue} (s)\")\n",
        "print(f\"Policy Iteration: {timePolicy} (s)\")\n",
        "valueTimes.append(timeValue)\n",
        "policyTimes.append(timePolicy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o83Qdetb5ZLN",
        "outputId": "dc10c133-ac25-4758-c1ac-599bfbdc6e5a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration: 0.13229155540466309 (s)\n",
            "Policy Iteration: 0.11452031135559082 (s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(FroLakv1, FroLakv1_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePRKRPkoTKKj",
        "outputId": "264c4a07-08e3-4aff-9bbb-4e02d78b6297"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 770/1000\n",
            "Average number of steps: 40.44285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrozenLake8x8-v1"
      ],
      "metadata": {
        "id": "7upAFdQa-dR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FroLak8x8 = gym.make('FrozenLake-v1', map_name=\"8x8\", render_mode=\"ansi\")"
      ],
      "metadata": {
        "id": "NuBw3r6r8JxZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FroLak8x8_policy, timePolicy = timeCompute(FroLak8x8, 'policy')\n",
        "FroLak8x8_value, timeValue = timeCompute(FroLak8x8, 'value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkXnMR5G_SG7",
        "outputId": "4bc73247-6dad-43de-dc86-4aca0f675166"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 27-th iteration.\n",
            "Policy evaluation: Converged at 91-th iteration.\n",
            "Policy evaluation: Converged at 92-th iteration.\n",
            "Policy evaluation: Converged at 86-th iteration.\n",
            "Policy evaluation: Converged at 90-th iteration.\n",
            "Policy evaluation: Converged at 92-th iteration.\n",
            "Policy evaluation: Converged at 95-th iteration.\n",
            "Policy evaluation: Converged at 100-th iteration.\n",
            "Policy evaluation: Converged at 112-th iteration.\n",
            "Policy evaluation: Converged at 117-th iteration.\n",
            "Policy iteration: Converged at 9-th iteration.\n",
            "Value iteration: Converged at 117-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Value Iteration: {timeValue} (s)\")\n",
        "print(f\"Policy Iteration: {timePolicy} (s)\")\n",
        "valueTimes.append(timeValue)\n",
        "policyTimes.append(timePolicy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6mMbEXf_r_B",
        "outputId": "c38affb1-6f9f-44cd-87a5-f2a16a09d625"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration: 0.6320934295654297 (s)\n",
            "Policy Iteration: 0.9953222274780273 (s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(FroLak8x8, FroLak8x8_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txpay7LWDS7b",
        "outputId": "c00d961f-46b4-4e18-c1bb-c849af3cce87"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 740/1000\n",
            "Average number of steps: 74.2581081081081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taxi-v3"
      ],
      "metadata": {
        "id": "HaAw1jCgAJYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Taxiv3 = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "WCXpHNCv_0bv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Taxiv3_policy, timePolicy = timeCompute(Taxiv3, 'policy')\n",
        "Taxiv3_value, timeValue = timeCompute(Taxiv3, 'value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "736ytPfhAVs7",
        "outputId": "dddbd0cc-e491-4c39-94e0-43bad77fa1c4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluation: Converged at 88-th iteration.\n",
            "Policy evaluation: Converged at 97-th iteration.\n",
            "Policy evaluation: Converged at 100-th iteration.\n",
            "Policy evaluation: Converged at 101-th iteration.\n",
            "Policy evaluation: Converged at 102-th iteration.\n",
            "Policy evaluation: Converged at 103-th iteration.\n",
            "Policy evaluation: Converged at 106-th iteration.\n",
            "Policy evaluation: Converged at 109-th iteration.\n",
            "Policy evaluation: Converged at 110-th iteration.\n",
            "Policy evaluation: Converged at 111-th iteration.\n",
            "Policy evaluation: Converged at 112-th iteration.\n",
            "Policy evaluation: Converged at 115-th iteration.\n",
            "Policy evaluation: Converged at 116-th iteration.\n",
            "Policy evaluation: Converged at 116-th iteration.\n",
            "Policy evaluation: Converged at 116-th iteration.\n",
            "Policy evaluation: Converged at 116-th iteration.\n",
            "Policy evaluation: Converged at 116-th iteration.\n",
            "Policy iteration: Converged at 16-th iteration.\n",
            "Value iteration: Converged at 116-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Value Iteration: {timeValue} (s)\")\n",
        "print(f\"Policy Iteration: {timePolicy} (s)\")\n",
        "valueTimes.append(timeValue)\n",
        "policyTimes.append(timePolicy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L084qblCAep9",
        "outputId": "37864610-4c84-4e39-b015-a5c8a775d60d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration: 6.629977703094482 (s)\n",
            "Policy Iteration: 15.605125427246094 (s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(Taxiv3, Taxiv3_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVm4LkRjDWts",
        "outputId": "103b2c83-e0f9-41e5-e896-a25065e83ee5",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nhận xét"
      ],
      "metadata": {
        "id": "V-aK7HoABKHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "averageValue = np.average(valueTimes)\n",
        "averagePolicy = np.average(policyTimes)"
      ],
      "metadata": {
        "id": "cHMj4jflAlMP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Giá trị thời gian trung bình của value iteration: {averageValue}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbnNAQa4ELaz",
        "outputId": "10eb9305-cef4-405c-f171-798b05261388"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Giá trị thời gian trung bình của value iteration: 2.464787562688192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Giá trị thời gian trung bình của value iteration: {averagePolicy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEhKzCHEM3D",
        "outputId": "4dc7e37e-6002-491d-b55a-6aa43de71021"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Giá trị thời gian trung bình của value iteration: 5.571655988693237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dựa trên kết quả thực nghiệm, ta có thể thấy rằng thời gian chạy trung bình của value iteration thấp hơn so với policy iteration (2,4648 giây so với 5,5717 giây). Nguyên nhân chính là do policy iteration phải thực hiện đánh giá chiến lược (policy evaluation) và trích xuất chiến lược (policy extraction) liên tục với mỗi policy để dự đoán được chiến lược tiếp theo.\n",
        "\n",
        "Tuy nhiên, mặc dù value iteration mất ít thời gian hơn để chạy, policy iteration luôn hội tụ nhanh hơn. Điều này là do mục tiêu cuối cùng của chúng ta vẫn là tìm kiếm chiến lược tối ưu, và policy iteration đánh giá chiến lược liên tục và sẽ trả về chiến lược tối ưu ngay khi nó được tìm thấy (khi *nextPolicy* trùng với *currentPolicy*). Trong khi đó, value iteration lại tìm chiến lược tối ưu dựa trên sự khác biệt cực nhỏ giữa V_values.\n",
        "\n",
        "Do đó, mặc dù trong các bài toán trên, policy iteration có thể tốn chi phí tính toán cao hơn với việc đánh giá chiến lược liên tục, nhưng trong các bài toán lớn hơn, policy iteration có thể thể hiện hiệu suất tốt hơn vì khả năng hội tụ chiến lược tối ưu nhanh hơn. Điều này làm cho policy iteration trở thành lựa chọn thích hợp hơn cho các bài toán lớn, khi chúng ta đặt ưu tiên vào sự hội tụ nhanh chóng của thuật toán.\n",
        "\n"
      ],
      "metadata": {
        "id": "db4kSoMLUI9A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72vzjg1laFjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}